{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL Project - CRNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux5dR2eHbcCI",
        "colab_type": "code",
        "outputId": "61fa8135-7f2f-4ca6-d422-f861dc398986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow  as tf\n",
        "from tensorflow.contrib.layers import fully_connected\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import pickle\n",
        "from statistics import mean\n",
        "import keras\n",
        "from keras.layers.core import Dense, Permute, Reshape\n",
        "from keras.layers.convolutional import Convolution2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.regularizers import l2\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFbV0cO0bl1V",
        "colab_type": "code",
        "outputId": "1b509f3d-0d51-4a07-e6a5-6895f4ee94df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "drive.mount('/content/Drive/',force_remount =  True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/Drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDMCPAx4bsTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dirpath = \"/content/Drive/My Drive/DL project/New Files/MFCC Data/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMj-dbnIb_m0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading training pickle files\n",
        "sound_lists_pkl_in = open(dirpath+\"ca_data_padded_tr.pkl\", \"rb\")\n",
        "ca_data_tr = pickle.load(sound_lists_pkl_in)\n",
        "\n",
        "sound_lists_pkl_in = open(dirpath+\"en_data_padded_tr.pkl\", \"rb\")\n",
        "en_data_tr = pickle.load(sound_lists_pkl_in)\n",
        "\n",
        "sound_lists_pkl_in = open(dirpath+\"fr_data_padded_tr.pkl\", \"rb\")\n",
        "fr_data_tr = pickle.load(sound_lists_pkl_in)\n",
        "\n",
        "sound_lists_pkl_in = open(dirpath+\"it_data_padded_tr.pkl\", \"rb\")\n",
        "it_data_tr = pickle.load(sound_lists_pkl_in)\n",
        "\n",
        "sound_lists_pkl_in = open(dirpath+\"de_data_padded_tr.pkl\", \"rb\")\n",
        "de_data_tr = pickle.load(sound_lists_pkl_in)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxHG9-wHcmfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading test pickle files\n",
        "sound_lists_pkl_in = open(dirpath+\"ca_data_padded_te.pkl\", \"rb\")\n",
        "ca_data_te = pickle.load(sound_lists_pkl_in)\n",
        "\n",
        "sound_lists_pkl_in = open(dirpath+\"en_data_padded_te.pkl\", \"rb\")\n",
        "en_data_te = pickle.load(sound_lists_pkl_in)\n",
        "\n",
        "sound_lists_pkl_in = open(dirpath+\"fr_data_padded_te.pkl\", \"rb\")\n",
        "fr_data_te = pickle.load(sound_lists_pkl_in)\n",
        "\n",
        "sound_lists_pkl_in = open(dirpath+\"it_data_padded_te.pkl\", \"rb\")\n",
        "it_data_te = pickle.load(sound_lists_pkl_in)\n",
        "\n",
        "sound_lists_pkl_in = open(dirpath+\"de_data_padded_te.pkl\", \"rb\")\n",
        "de_data_te = pickle.load(sound_lists_pkl_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTewX5BTcsHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_data_tr = np.concatenate((ca_data_tr, en_data_tr, fr_data_tr, it_data_tr, de_data_tr), axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYe44TBTczm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_data_te = np.concatenate((ca_data_te, en_data_te, fr_data_te, it_data_te, de_data_te), axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62U6KWn3dS_5",
        "colab_type": "code",
        "outputId": "e071a3df-686b-4ca5-ebb2-0edec58e97f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(final_data_tr.shape)\n",
        "print(final_data_te.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 2)\n",
            "(2500, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBvPb6rDfZ_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del(de_data_te)\n",
        "del(ca_data_te)\n",
        "del(en_data_te)\n",
        "del(fr_data_te)\n",
        "del(it_data_te)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2g5oMzjflYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del(de_data_tr)\n",
        "del(ca_data_tr)\n",
        "del(en_data_tr)\n",
        "del(fr_data_tr)\n",
        "del(it_data_tr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEb3rX0psnMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# final_data_tr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRt82WVGs4iG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.shuffle(final_data_tr)\n",
        "np.random.shuffle(final_data_te)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8YcG4vts72w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data_padded_pkl = open(dirpath+\"final_data_tr.pkl\", \"wb\")\n",
        "# pickle.dump(final_data_tr, data_padded_pkl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MgEkHEwuuAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data_padded_pkl = open(dirpath+\"final_data_te.pkl\", \"wb\")\n",
        "# pickle.dump(final_data_te, data_padded_pkl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbJOywlP8j1w",
        "colab_type": "text"
      },
      "source": [
        "### Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qZ31OCn8ll5",
        "colab_type": "code",
        "outputId": "72584d7b-8942-4dd5-e242-cca2f656d3ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "labels_tr =[]\n",
        "for i in range(len(final_data_tr)):\n",
        "\n",
        "      labels_tr.append(final_data_tr[i][1])  \n",
        "\n",
        "labels_tr = np.array(labels_tr)\n",
        "labels_tr\n",
        "\n",
        "classnames, indices = np.unique(labels_tr, return_inverse=True)\n",
        "print(indices)\n",
        "\n",
        "final_labels_tr = indices"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4 2 0 ... 2 0 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyrKk9g68lEh",
        "colab_type": "code",
        "outputId": "a440a100-2ec1-4b63-b860-dc0862028876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "labels_te =[]\n",
        "for i in range(len(final_data_te)):\n",
        "\n",
        "      labels_te.append(final_data_te[i][1])  \n",
        "\n",
        "labels_te = np.array(labels_te)\n",
        "labels_te\n",
        "\n",
        "classnames, indices = np.unique(labels_te, return_inverse=True)\n",
        "print(indices)\n",
        "\n",
        "final_labels_te = indices"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3 2 1 ... 1 3 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jALa80G-fkc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvB12R0l_TO3",
        "colab_type": "code",
        "outputId": "e046036a-7095-467b-d258-437d93f26373",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "final_X_tr =[]\n",
        "for i in range(len(final_data_tr)):\n",
        "\n",
        "      final_X_tr.append(final_data_tr[i][0])  \n",
        "\n",
        "final_X_tr = np.array(final_X_tr)\n",
        "final_X_tr[0].shape\n",
        "\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(972, 40)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQOkHS85_wa2",
        "colab_type": "code",
        "outputId": "26c0558b-5f03-4a22-c83e-b3957f7cb362",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "final_X_te =[]\n",
        "for i in range(len(final_data_te)):\n",
        "\n",
        "      final_X_te.append(final_data_te[i][0])  \n",
        "\n",
        "final_X_te = np.array(final_X_te)\n",
        "final_X_te[0].shape\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(972, 40)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7khvS8a8Wg1",
        "colab_type": "text"
      },
      "source": [
        "### Keras Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fH90G2JyxCtE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers.core import Dense, Permute, Reshape, Flatten \n",
        "from keras.layers.convolutional import Convolution2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers import Dropout, TimeDistributed\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYoOypuT8ZCU",
        "colab_type": "code",
        "outputId": "49eaafb2-6b01-454e-bce9-a3c48c888a4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "model = Sequential()\n",
        "weight_decay = 0.001\n",
        "\n",
        "model.add(Convolution2D(32, kernel_size=(4,4), strides=(1,1), activation=\"relu\", padding=\"same\", kernel_regularizer=l2(weight_decay), input_shape=(972,40,1)))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(64, kernel_size=(4,4), strides=(1,1), activation=\"relu\", kernel_regularizer=l2(weight_decay), padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"))\n",
        "\n",
        "\n",
        "\n",
        "model.add(Convolution2D(128, kernel_size=(4,4), strides=(1,1), activation=\"relu\", kernel_regularizer=l2(weight_decay), padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(256, kernel_size=(4,4), strides=(1,1), activation=\"relu\", kernel_regularizer=l2(weight_decay), padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"))\n",
        "\n",
        "model.add(Convolution2D(512, kernel_size=(4,4), strides=(1,1), activation=\"relu\", kernel_regularizer=l2(weight_decay), padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"))\n",
        "\n",
        "\n",
        "model.add(Convolution2D(512, kernel_size=(4,4), strides=(1,1), activation=\"relu\", kernel_regularizer=l2(weight_decay), padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding=\"same\"))\n",
        "\n",
        "print(model.layers[-1].output_shape)\n",
        "\n",
        "#model.add(Permute((2,1,3)))\n",
        "\n",
        "print(model.layers[-1].output_shape)\n",
        "\n",
        "bs, x, y, c = model.layers[-1].output_shape\n",
        "\n",
        "model.add(Reshape([x,y*c]))\n",
        "\n",
        "print(model.layers[-1].output_shape)\n",
        "\n",
        "#model.add(LSTM(256, activation=\"tanh\", return_sequences=True))\n",
        "\n",
        "model.add(LSTM(512, activation=\"relu\",dropout = 0.7, return_sequences=False))\n",
        "\n",
        "# model.add(Flatten())\n",
        "\n",
        "# model.add(Dense(256, activation=\"relu\"))\n",
        "\n",
        "# model.add(Dropout(rate=0.7))\n",
        "\n",
        "model.add(Dense(5, activation=\"softmax\"))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "(None, 16, 1, 512)\n",
            "(None, 16, 1, 512)\n",
            "(None, 16, 512)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr5bUTx5vk3-",
        "colab_type": "code",
        "outputId": "89078492-77c4-4938-ecdc-f5e0f286d713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 972, 40, 32)       544       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 972, 40, 32)       128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 486, 20, 32)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 486, 20, 64)       32832     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 486, 20, 64)       256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 243, 10, 64)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 243, 10, 128)      131200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 243, 10, 128)      512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 122, 5, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 122, 5, 256)       524544    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 122, 5, 256)       1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 61, 3, 256)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 61, 3, 512)        2097664   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 61, 3, 512)        2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 31, 2, 512)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 31, 2, 512)        4194816   \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 31, 2, 512)        2048      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 16, 1, 512)        0         \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 16, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 2565      \n",
            "=================================================================\n",
            "Total params: 9,089,381\n",
            "Trainable params: 9,086,373\n",
            "Non-trainable params: 3,008\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvbPfFuCAFlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del(final_data_tr)\n",
        "del(final_data_te)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBTHNw0qAgdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx = np.array(final_X_tr[:8000])\n",
        "cvx = np.array(final_X_tr[8000:])\n",
        "ty = np.array(final_labels_tr[:8000])\n",
        "cvy = np.array(final_labels_tr[8000:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPa_Hz2yBNxt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del(final_labels_tr)\n",
        "del(final_X_tr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-YaBGQcBUln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cvy = to_categorical(cvy)\n",
        "ty = to_categorical(ty)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVhlJEtUCeh4",
        "colab_type": "code",
        "outputId": "c44f06e2-ae57-4cb7-aac6-642a3e749b63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "ty"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [1., 0., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [1., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6IcH27SAixd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "             optimizer=keras.optimizers.adam(lr=0.0002),\n",
        "             metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDHTkxA-BugR",
        "colab_type": "code",
        "outputId": "d3980d9b-3866-4df3-fe7c-6a527473fe41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5324
        }
      },
      "source": [
        "#Fitting the model\n",
        "model.fit(tx.reshape(-1,972,40,1), ty,\n",
        "          batch_size=300,\n",
        "          epochs=150,\n",
        "          verbose=1,\n",
        "          validation_data=(cvx.reshape(-1,972,40,1),cvy)\n",
        "         )\n",
        "          "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/150\n",
            "8000/8000 [==============================] - 59s 7ms/step - loss: 2.5358 - acc: 0.4371 - val_loss: 2.0441 - val_acc: 0.6585\n",
            "Epoch 2/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 1.9435 - acc: 0.6244 - val_loss: 1.8251 - val_acc: 0.6525\n",
            "Epoch 3/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 1.7003 - acc: 0.7024 - val_loss: 1.6286 - val_acc: 0.7150\n",
            "Epoch 4/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 1.5762 - acc: 0.7464 - val_loss: 1.6220 - val_acc: 0.7140\n",
            "Epoch 5/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 1.4871 - acc: 0.7921 - val_loss: 1.5064 - val_acc: 0.7830\n",
            "Epoch 6/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 1.4401 - acc: 0.8095 - val_loss: 1.7139 - val_acc: 0.7065\n",
            "Epoch 7/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 1.3823 - acc: 0.8336 - val_loss: 1.6861 - val_acc: 0.7010\n",
            "Epoch 8/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 1.3150 - acc: 0.8576 - val_loss: 1.6885 - val_acc: 0.7620\n",
            "Epoch 9/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 1.2633 - acc: 0.8761 - val_loss: 2.2918 - val_acc: 0.5900\n",
            "Epoch 10/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 1.2231 - acc: 0.8845 - val_loss: 1.6742 - val_acc: 0.7140\n",
            "Epoch 11/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 1.1867 - acc: 0.8971 - val_loss: 1.5007 - val_acc: 0.7590\n",
            "Epoch 12/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 1.1189 - acc: 0.9141 - val_loss: 1.8710 - val_acc: 0.7445\n",
            "Epoch 13/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 1.0811 - acc: 0.9253 - val_loss: 1.3094 - val_acc: 0.8425\n",
            "Epoch 14/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 1.0245 - acc: 0.9417 - val_loss: 1.8097 - val_acc: 0.7215\n",
            "Epoch 15/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.9613 - acc: 0.9604 - val_loss: 1.8606 - val_acc: 0.6985\n",
            "Epoch 16/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.9536 - acc: 0.9560 - val_loss: 1.2297 - val_acc: 0.8560\n",
            "Epoch 17/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.9188 - acc: 0.9637 - val_loss: 1.5164 - val_acc: 0.8010\n",
            "Epoch 18/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.8912 - acc: 0.9672 - val_loss: 1.3068 - val_acc: 0.8475\n",
            "Epoch 19/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.8565 - acc: 0.9751 - val_loss: 1.2324 - val_acc: 0.8715\n",
            "Epoch 20/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.8349 - acc: 0.9774 - val_loss: 1.6475 - val_acc: 0.8155\n",
            "Epoch 21/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.8276 - acc: 0.9769 - val_loss: 1.1893 - val_acc: 0.8690\n",
            "Epoch 22/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.7987 - acc: 0.9825 - val_loss: 1.1147 - val_acc: 0.8820\n",
            "Epoch 23/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.8053 - acc: 0.9751 - val_loss: 1.5662 - val_acc: 0.7765\n",
            "Epoch 24/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.7738 - acc: 0.9820 - val_loss: 1.4004 - val_acc: 0.8450\n",
            "Epoch 25/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.7429 - acc: 0.9881 - val_loss: 1.4259 - val_acc: 0.8240\n",
            "Epoch 26/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.7343 - acc: 0.9880 - val_loss: 2.4309 - val_acc: 0.7380\n",
            "Epoch 27/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.7177 - acc: 0.9871 - val_loss: 1.3016 - val_acc: 0.8600\n",
            "Epoch 28/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.6888 - acc: 0.9935 - val_loss: 1.2144 - val_acc: 0.8665\n",
            "Epoch 29/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.6749 - acc: 0.9939 - val_loss: 1.1299 - val_acc: 0.8895\n",
            "Epoch 30/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.6582 - acc: 0.9948 - val_loss: 1.4022 - val_acc: 0.8645\n",
            "Epoch 31/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.6413 - acc: 0.9960 - val_loss: 1.1563 - val_acc: 0.8885\n",
            "Epoch 32/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.6304 - acc: 0.9968 - val_loss: 1.2545 - val_acc: 0.8680\n",
            "Epoch 33/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.6115 - acc: 0.9979 - val_loss: 1.2916 - val_acc: 0.8855\n",
            "Epoch 34/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.6035 - acc: 0.9971 - val_loss: 1.3700 - val_acc: 0.8625\n",
            "Epoch 35/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.6116 - acc: 0.9921 - val_loss: 1.0866 - val_acc: 0.8865\n",
            "Epoch 36/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.6054 - acc: 0.9875 - val_loss: 1.4054 - val_acc: 0.8320\n",
            "Epoch 37/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.5895 - acc: 0.9913 - val_loss: 1.2588 - val_acc: 0.8425\n",
            "Epoch 38/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.5776 - acc: 0.9921 - val_loss: 2.0010 - val_acc: 0.7530\n",
            "Epoch 39/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.5617 - acc: 0.9940 - val_loss: 1.3355 - val_acc: 0.8525\n",
            "Epoch 40/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.5455 - acc: 0.9959 - val_loss: 1.4567 - val_acc: 0.8430\n",
            "Epoch 41/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.5554 - acc: 0.9908 - val_loss: 1.0502 - val_acc: 0.8900\n",
            "Epoch 42/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.5401 - acc: 0.9901 - val_loss: 1.0695 - val_acc: 0.8925\n",
            "Epoch 43/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.5172 - acc: 0.9965 - val_loss: 1.3051 - val_acc: 0.8475\n",
            "Epoch 44/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.5072 - acc: 0.9964 - val_loss: 1.2502 - val_acc: 0.8725\n",
            "Epoch 45/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.5010 - acc: 0.9945 - val_loss: 1.1250 - val_acc: 0.8815\n",
            "Epoch 46/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.4955 - acc: 0.9935 - val_loss: 1.2218 - val_acc: 0.8535\n",
            "Epoch 47/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.4792 - acc: 0.9973 - val_loss: 1.8060 - val_acc: 0.8105\n",
            "Epoch 48/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.4643 - acc: 0.9988 - val_loss: 1.4456 - val_acc: 0.8520\n",
            "Epoch 49/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.4539 - acc: 0.9988 - val_loss: 0.9543 - val_acc: 0.8915\n",
            "Epoch 50/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.4532 - acc: 0.9965 - val_loss: 0.9804 - val_acc: 0.8985\n",
            "Epoch 51/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.4395 - acc: 0.9979 - val_loss: 1.3203 - val_acc: 0.8620\n",
            "Epoch 52/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.4338 - acc: 0.9970 - val_loss: 0.9880 - val_acc: 0.8985\n",
            "Epoch 53/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.4268 - acc: 0.9961 - val_loss: 1.3310 - val_acc: 0.8425\n",
            "Epoch 54/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.4256 - acc: 0.9951 - val_loss: 3.8798 - val_acc: 0.6450\n",
            "Epoch 55/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.4186 - acc: 0.9950 - val_loss: 1.2384 - val_acc: 0.8460\n",
            "Epoch 56/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.4064 - acc: 0.9964 - val_loss: 0.9597 - val_acc: 0.8810\n",
            "Epoch 57/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3980 - acc: 0.9971 - val_loss: 1.1491 - val_acc: 0.8585\n",
            "Epoch 58/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3877 - acc: 0.9980 - val_loss: 1.3472 - val_acc: 0.8530\n",
            "Epoch 59/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3815 - acc: 0.9975 - val_loss: 0.8972 - val_acc: 0.8955\n",
            "Epoch 60/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3728 - acc: 0.9981 - val_loss: 0.8866 - val_acc: 0.8975\n",
            "Epoch 61/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3708 - acc: 0.9966 - val_loss: 1.9807 - val_acc: 0.8120\n",
            "Epoch 62/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3682 - acc: 0.9953 - val_loss: 0.9917 - val_acc: 0.8890\n",
            "Epoch 63/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.3626 - acc: 0.9958 - val_loss: 0.9442 - val_acc: 0.8930\n",
            "Epoch 64/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3583 - acc: 0.9950 - val_loss: 0.9845 - val_acc: 0.8940\n",
            "Epoch 65/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3489 - acc: 0.9969 - val_loss: 0.8433 - val_acc: 0.9040\n",
            "Epoch 66/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3422 - acc: 0.9968 - val_loss: 1.0273 - val_acc: 0.8740\n",
            "Epoch 67/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3321 - acc: 0.9988 - val_loss: 1.3731 - val_acc: 0.8385\n",
            "Epoch 68/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3243 - acc: 0.9984 - val_loss: 2.0820 - val_acc: 0.7635\n",
            "Epoch 69/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3171 - acc: 0.9989 - val_loss: 1.0204 - val_acc: 0.8890\n",
            "Epoch 70/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3093 - acc: 0.9994 - val_loss: 0.9087 - val_acc: 0.9025\n",
            "Epoch 71/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3045 - acc: 0.9995 - val_loss: 1.2460 - val_acc: 0.8570\n",
            "Epoch 72/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2970 - acc: 0.9995 - val_loss: 1.0337 - val_acc: 0.8790\n",
            "Epoch 73/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2958 - acc: 0.9980 - val_loss: 1.5223 - val_acc: 0.8215\n",
            "Epoch 74/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2957 - acc: 0.9963 - val_loss: 1.5974 - val_acc: 0.8350\n",
            "Epoch 75/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.3029 - acc: 0.9934 - val_loss: 1.8859 - val_acc: 0.7830\n",
            "Epoch 76/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2965 - acc: 0.9949 - val_loss: 1.2809 - val_acc: 0.8615\n",
            "Epoch 77/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2908 - acc: 0.9948 - val_loss: 1.1569 - val_acc: 0.8390\n",
            "Epoch 78/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2827 - acc: 0.9968 - val_loss: 1.4537 - val_acc: 0.8460\n",
            "Epoch 79/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2763 - acc: 0.9975 - val_loss: 1.3717 - val_acc: 0.8410\n",
            "Epoch 80/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2831 - acc: 0.9950 - val_loss: 1.4277 - val_acc: 0.8015\n",
            "Epoch 81/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2745 - acc: 0.9959 - val_loss: 0.7460 - val_acc: 0.8990\n",
            "Epoch 82/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2659 - acc: 0.9968 - val_loss: 1.1757 - val_acc: 0.8560\n",
            "Epoch 83/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2598 - acc: 0.9975 - val_loss: 1.2567 - val_acc: 0.8240\n",
            "Epoch 84/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2534 - acc: 0.9983 - val_loss: 0.9511 - val_acc: 0.8760\n",
            "Epoch 85/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2483 - acc: 0.9989 - val_loss: 0.7189 - val_acc: 0.9070\n",
            "Epoch 86/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2458 - acc: 0.9986 - val_loss: 0.7152 - val_acc: 0.9060\n",
            "Epoch 87/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2595 - acc: 0.9931 - val_loss: 0.6826 - val_acc: 0.8965\n",
            "Epoch 88/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.2507 - acc: 0.9945 - val_loss: 0.6774 - val_acc: 0.9150\n",
            "Epoch 89/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.2447 - acc: 0.9959 - val_loss: 0.9927 - val_acc: 0.8650\n",
            "Epoch 90/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2452 - acc: 0.9958 - val_loss: 0.9056 - val_acc: 0.8830\n",
            "Epoch 91/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2363 - acc: 0.9969 - val_loss: 0.6798 - val_acc: 0.9010\n",
            "Epoch 92/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2609 - acc: 0.9890 - val_loss: 1.2996 - val_acc: 0.8205\n",
            "Epoch 93/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.2583 - acc: 0.9920 - val_loss: 1.3726 - val_acc: 0.8255\n",
            "Epoch 94/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2489 - acc: 0.9954 - val_loss: 0.8149 - val_acc: 0.8920\n",
            "Epoch 95/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2532 - acc: 0.9939 - val_loss: 0.6796 - val_acc: 0.9010\n",
            "Epoch 96/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2393 - acc: 0.9971 - val_loss: 0.8502 - val_acc: 0.8780\n",
            "Epoch 97/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.2362 - acc: 0.9959 - val_loss: 0.7750 - val_acc: 0.8975\n",
            "Epoch 98/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.2275 - acc: 0.9980 - val_loss: 0.7561 - val_acc: 0.9100\n",
            "Epoch 99/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2284 - acc: 0.9968 - val_loss: 1.0383 - val_acc: 0.8680\n",
            "Epoch 100/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2220 - acc: 0.9984 - val_loss: 0.6425 - val_acc: 0.9125\n",
            "Epoch 101/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.2162 - acc: 0.9986 - val_loss: 1.0031 - val_acc: 0.8770\n",
            "Epoch 102/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2151 - acc: 0.9976 - val_loss: 0.6635 - val_acc: 0.9010\n",
            "Epoch 103/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2088 - acc: 0.9988 - val_loss: 0.5889 - val_acc: 0.9185\n",
            "Epoch 104/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2075 - acc: 0.9981 - val_loss: 0.5788 - val_acc: 0.9210\n",
            "Epoch 105/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.2022 - acc: 0.9989 - val_loss: 1.1816 - val_acc: 0.8550\n",
            "Epoch 106/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.2017 - acc: 0.9986 - val_loss: 0.7135 - val_acc: 0.9020\n",
            "Epoch 107/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2039 - acc: 0.9980 - val_loss: 0.6778 - val_acc: 0.9050\n",
            "Epoch 108/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.1945 - acc: 0.9989 - val_loss: 0.6717 - val_acc: 0.9125\n",
            "Epoch 109/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.1896 - acc: 0.9993 - val_loss: 1.1197 - val_acc: 0.8525\n",
            "Epoch 110/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1851 - acc: 0.9999 - val_loss: 0.7668 - val_acc: 0.9040\n",
            "Epoch 111/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1813 - acc: 1.0000 - val_loss: 0.7525 - val_acc: 0.9075\n",
            "Epoch 112/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.1781 - acc: 1.0000 - val_loss: 0.5898 - val_acc: 0.9265\n",
            "Epoch 113/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1752 - acc: 0.9999 - val_loss: 0.6761 - val_acc: 0.9095\n",
            "Epoch 114/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1719 - acc: 1.0000 - val_loss: 0.6553 - val_acc: 0.9170\n",
            "Epoch 115/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1705 - acc: 0.9991 - val_loss: 0.7870 - val_acc: 0.8930\n",
            "Epoch 116/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.1710 - acc: 0.9980 - val_loss: 0.8761 - val_acc: 0.8935\n",
            "Epoch 117/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1780 - acc: 0.9958 - val_loss: 0.8626 - val_acc: 0.8840\n",
            "Epoch 118/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1755 - acc: 0.9964 - val_loss: 1.1060 - val_acc: 0.8435\n",
            "Epoch 119/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1872 - acc: 0.9944 - val_loss: 7.6566 - val_acc: 0.3885\n",
            "Epoch 120/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.2693 - acc: 0.9728 - val_loss: 2.6049 - val_acc: 0.6815\n",
            "Epoch 121/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.2270 - acc: 0.9921 - val_loss: 0.9123 - val_acc: 0.8465\n",
            "Epoch 122/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.2052 - acc: 0.9976 - val_loss: 0.8869 - val_acc: 0.8805\n",
            "Epoch 123/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.2008 - acc: 0.9975 - val_loss: 0.7755 - val_acc: 0.8985\n",
            "Epoch 124/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.1948 - acc: 0.9984 - val_loss: 0.7081 - val_acc: 0.9185\n",
            "Epoch 125/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.1915 - acc: 0.9980 - val_loss: 0.6294 - val_acc: 0.9230\n",
            "Epoch 126/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1868 - acc: 0.9989 - val_loss: 0.8984 - val_acc: 0.8775\n",
            "Epoch 127/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1888 - acc: 0.9971 - val_loss: 0.6580 - val_acc: 0.9200\n",
            "Epoch 128/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.1872 - acc: 0.9964 - val_loss: 0.7370 - val_acc: 0.8845\n",
            "Epoch 129/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1820 - acc: 0.9975 - val_loss: 0.7184 - val_acc: 0.9040\n",
            "Epoch 130/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.1831 - acc: 0.9966 - val_loss: 0.7122 - val_acc: 0.8980\n",
            "Epoch 131/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.1734 - acc: 0.9986 - val_loss: 0.6296 - val_acc: 0.9110\n",
            "Epoch 132/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.1683 - acc: 0.9995 - val_loss: 0.7564 - val_acc: 0.9080\n",
            "Epoch 133/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.1659 - acc: 0.9994 - val_loss: 0.7262 - val_acc: 0.9040\n",
            "Epoch 134/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1630 - acc: 0.9994 - val_loss: 0.6699 - val_acc: 0.9160\n",
            "Epoch 135/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1608 - acc: 0.9993 - val_loss: 0.6959 - val_acc: 0.9015\n",
            "Epoch 136/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.1600 - acc: 0.9990 - val_loss: 0.8376 - val_acc: 0.8915\n",
            "Epoch 137/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.1567 - acc: 0.9995 - val_loss: 0.8465 - val_acc: 0.8855\n",
            "Epoch 138/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.1526 - acc: 1.0000 - val_loss: 0.8869 - val_acc: 0.8945\n",
            "Epoch 139/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1497 - acc: 1.0000 - val_loss: 0.7330 - val_acc: 0.9180\n",
            "Epoch 140/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.1470 - acc: 1.0000 - val_loss: 0.6802 - val_acc: 0.9245\n",
            "Epoch 141/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.1444 - acc: 1.0000 - val_loss: 0.6676 - val_acc: 0.9250\n",
            "Epoch 142/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1420 - acc: 1.0000 - val_loss: 0.6442 - val_acc: 0.9275\n",
            "Epoch 143/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1397 - acc: 1.0000 - val_loss: 0.7132 - val_acc: 0.9125\n",
            "Epoch 144/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.1387 - acc: 0.9996 - val_loss: 1.2596 - val_acc: 0.8475\n",
            "Epoch 145/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.1405 - acc: 0.9989 - val_loss: 1.4439 - val_acc: 0.8095\n",
            "Epoch 146/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.1434 - acc: 0.9974 - val_loss: 0.8700 - val_acc: 0.9020\n",
            "Epoch 147/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2708 - acc: 0.9630 - val_loss: 10.3108 - val_acc: 0.2585\n",
            "Epoch 148/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.2528 - acc: 0.9795 - val_loss: 0.7144 - val_acc: 0.8705\n",
            "Epoch 149/150\n",
            "8000/8000 [==============================] - 38s 5ms/step - loss: 0.2195 - acc: 0.9916 - val_loss: 1.2315 - val_acc: 0.8075\n",
            "Epoch 150/150\n",
            "8000/8000 [==============================] - 39s 5ms/step - loss: 0.2115 - acc: 0.9931 - val_loss: 0.6524 - val_acc: 0.9110\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6d05b181d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxwQ2-uGChAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(cvx.reshape(-1,972,40,1))\n",
        "matrix = confusion_matrix(cvy.argmax(axis=1), y_pred.argmax(axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uE621Q7w_0c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "10ba2b5d-28cc-48f1-c3db-9540e81ff65f"
      },
      "source": [
        "matrix"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[389,   9,   0,   2,   0],\n",
              "       [ 22, 361,   0,   0,   0],\n",
              "       [ 22,  13, 317,  44,   1],\n",
              "       [ 19,  14,  26, 342,   0],\n",
              "       [  5,   0,   0,   1, 413]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip1C1f2VS3AO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8e1f02cc-3ab5-403e-ab5a-6e75a398746d"
      },
      "source": [
        "np.unique(ty, return_counts=True)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0., 1.], dtype=float32), array([32000,  8000]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjj3eEuETlmY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1ad4ce9e-bafb-427d-d630-db3621c21aba"
      },
      "source": [
        "np.unique(cvy, return_counts=True)\n",
        "  "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0., 1.], dtype=float32), array([8000, 2000]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqIldBs6UVuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}